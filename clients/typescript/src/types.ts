// Minimal type definitions for JSR publishing
// Full spec available at: https://github.com/mieweb/ozwellai-api/tree/main/spec

/**
 * Request object for chat completion API calls.
 * Compatible with OpenAI's chat completion format.
 */
export interface ChatCompletionRequest {
  /** ID of the model to use */
  model: string;
  /** A list of messages comprising the conversation so far */
  messages: Array<{
    /** The role of the messages author */
    role: 'system' | 'user' | 'assistant' | 'function' | 'tool';
    /** The contents of the message */
    content: string;
    /** The name of the author of this message */
    name?: string;
  }>;
  /** The maximum number of tokens to generate in the chat completion */
  max_tokens?: number;
  /** What sampling temperature to use, between 0 and 2 */
  temperature?: number;
  /** An alternative to sampling with temperature, called nucleus sampling */
  top_p?: number;
  /** How many chat completion choices to generate for each input message */
  n?: number;
  /** If set, partial message deltas will be sent, like in ChatGPT */
  stream?: boolean;
  /** Up to 4 sequences where the API will stop generating further tokens */
  stop?: string | string[];
  /** Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far */
  presence_penalty?: number;
  /** Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far */
  frequency_penalty?: number;
  /** Modify the likelihood of specified tokens appearing in the completion */
  logit_bias?: Record<string, number>;
  /** A unique identifier representing your end-user */
  user?: string;
}

/**
 * Response object from chat completion API calls.
 * Contains the generated message and usage information.
 */
export interface ChatCompletionResponse {
  /** A unique identifier for the chat completion */
  id: string;
  /** The object type, which is always "chat.completion" */
  object: 'chat.completion';
  /** The Unix timestamp (in seconds) of when the chat completion was created */
  created: number;
  /** The model used for the chat completion */
  model: string;
  /** A list of chat completion choices */
  choices: Array<{
    /** The index of the choice in the list of choices */
    index: number;
    /** A chat completion message generated by the model */
    message: {
      /** The role of the author of this message */
      role: 'assistant';
      /** The contents of the message */
      content: string;
    };
    /** The reason the model stopped generating tokens */
    finish_reason: 'stop' | 'length' | 'function_call' | 'content_filter' | null;
  }>;
  /** Usage statistics for the completion request */
  usage: {
    /** Number of tokens in the prompt */
    prompt_tokens: number;
    /** Number of tokens in the generated completion */
    completion_tokens: number;
    /** Total number of tokens used in the request (prompt + completion) */
    total_tokens: number;
  };
}

/**
 * Request object for embedding API calls.
 * Used to get vector representations of text.
 */
export interface EmbeddingRequest {
  /** ID of the model to use */
  model: string;
  /** Input text to embed, encoded as a string or array of strings */
  input: string | string[];
  /** A unique identifier representing your end-user */
  user?: string;
}

/**
 * Response object from embedding API calls.
 * Contains the vector embeddings for the input text.
 */
export interface EmbeddingResponse {
  /** The object type, which is always "list" */
  object: 'list';
  /** The list of embeddings generated by the model */
  data: Array<{
    /** The object type, which is always "embedding" */
    object: 'embedding';
    /** The embedding vector, which is a list of floats */
    embedding: number[];
    /** The index of the embedding in the list of embeddings */
    index: number;
  }>;
  /** The name of the model used to generate the embedding */
  model: string;
  /** The usage information for the request */
  usage: {
    /** The number of tokens used by the prompt */
    prompt_tokens: number;
    /** The total number of tokens used by the request */
    total_tokens: number;
  };
}

/**
 * Response object containing available models.
 * Lists all models available for use with the API.
 */
export interface ModelsListResponse {
  /** The object type, which is always "list" */
  object: 'list';
  /** The list of model objects */
  data: Array<{
    /** The model identifier, which can be referenced in the API endpoints */
    id: string;
    /** The object type, which is always "model" */
    object: 'model';
    /** The Unix timestamp (in seconds) when the model was created */
    created: number;
    /** The organization that owns the model */
    owned_by: string;
  }>;
}

/**
 * Represents an uploaded file.
 * Contains metadata about files uploaded to the API.
 */
export interface FileObject {
  /** The file identifier, which can be referenced in the API endpoints */
  id: string;
  /** The object type, which is always "file" */
  object: 'file';
  /** The size of the file, in bytes */
  bytes: number;
  /** The Unix timestamp (in seconds) for when the file was created */
  created_at: number;
  /** The name of the file */
  filename: string;
  /** The intended purpose of the file */
  purpose: string;
}

/**
 * Response object containing a list of files.
 * Lists all files that have been uploaded.
 */
export interface FileListResponse {
  /** The object type, which is always "list" */
  object: 'list';
  /** The list of file objects */
  data: FileObject[];
}

/**
 * Request object for text completion API calls.
 * Legacy completion format for generating text.
 */
export interface ResponseRequest {
  /** The prompt(s) to generate completions for */
  prompt: string;
  /** The maximum number of tokens to generate in the completion */
  max_tokens?: number;
  /** What sampling temperature to use, between 0 and 2 */
  temperature?: number;
  /** An alternative to sampling with temperature, called nucleus sampling */
  top_p?: number;
  /** How many completions to generate for each prompt */
  n?: number;
  /** Whether to stream back partial progress */
  stream?: boolean;
  /** Include the log probabilities on the logprobs most likely tokens */
  logprobs?: number;
  /** Echo back the prompt in addition to the completion */
  echo?: boolean;
  /** Up to 4 sequences where the API will stop generating further tokens */
  stop?: string | string[];
  /** Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far */
  presence_penalty?: number;
  /** Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far */
  frequency_penalty?: number;
  /** Generates best_of completions server-side and returns the "best" */
  best_of?: number;
  /** Modify the likelihood of specified tokens appearing in the completion */
  logit_bias?: Record<string, number>;
  /** A unique identifier representing your end-user */
  user?: string;
}

/**
 * Response object from text completion API calls.
 * Contains the generated text and usage information.
 */
export interface Response {
  /** A unique identifier for the completion */
  id: string;
  /** The object type, which is always "text_completion" */
  object: 'text_completion';
  /** The Unix timestamp (in seconds) of when the completion was created */
  created: number;
  /** The model used for completion */
  model: string;
  /** The list of completion choices sorted by their respective log probabilities */
  choices: Array<{
    /** The generated text */
    text: string;
    /** The index of the choice in the list of choices */
    index: number;
    /** Log probability information for the choice tokens */
    logprobs: null | {
      /** The tokens chosen by the completion API */
      tokens: string[];
      /** The log probability of each token in tokens */
      token_logprobs: number[];
      /** The top log probabilities for each position */
      top_logprobs: Array<Record<string, number>>;
      /** The character offset from the start of the returned text for each token */
      text_offset: number[];
    };
    /** The reason the model stopped generating tokens */
    finish_reason: 'stop' | 'length' | 'content_filter' | null;
  }>;
  /** Usage statistics for the completion request */
  usage: {
    /** Number of tokens in the prompt */
    prompt_tokens: number;
    /** Number of tokens in the generated completion */
    completion_tokens: number;
    /** Total number of tokens used in the request (prompt + completion) */
    total_tokens: number;
  };
}
